{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipynb.fs.full.dataset import load_conversations\n",
    "from ipynb.fs.full.word2vec_implementation import build_word2vec\n",
    "from ipynb.fs.full.dataset import preprocess_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path_persona = \"/home/parastoo/Desktop/t2t/Seq2seqChatbots/data_dir/DailyDialog/test_repo/trainSource.txt\"\n",
    "data_path_persona_target = \"/home/parastoo/Desktop/t2t/Seq2seqChatbots/data_dir/DailyDialog/test_repo/trainSource.txt\"\n",
    "questions , answers = load_conversations(data_path_persona , data_path_persona_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path_rdany = \"/home/parastoo/Downloads/human_text.txt\"\n",
    "data_path_rdany_target = \"/home/parastoo/Downloads/robot_text.txt\"\n",
    "human , robot = load_conversations(data_path_rdany , data_path_rdany_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_samples = len(questions)\n",
    "questions = questions[:int(len_samples/3)]\n",
    "answers = answers[:int(len_samples/3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37045, 2)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "data = {'questions': questions , 'answers':answers}\n",
    "pd.DataFrame(data).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13348"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec = {}\n",
    "vocab , inv_vocab , word2vec  = build_word2vec(questions , answers ,word2vec)\n",
    "vocab , inv_vocab , word2vec = build_word2vec(human , robot , word2vec)\n",
    "len(word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "def build_padded_encoder_input_data(questions , answers , vocab , inv_vocab):\n",
    "    encoder_inp = []\n",
    "    for line in questions:\n",
    "        lst = []\n",
    "        for word in line.split():\n",
    "            if word not in vocab:\n",
    "                lst.append(vocab['<OUT>'])\n",
    "            else:\n",
    "                lst.append(vocab[word])\n",
    "        encoder_inp.append(lst)\n",
    "    encoder_inp = pad_sequences(encoder_inp, 40, padding='post', truncating='post')\n",
    "    return encoder_inp\n",
    "\n",
    "def build_padded_decoder_input_data(questions, answers , vocab , inv_vocab):    \n",
    "    from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "    decoder_inp = []\n",
    "    for line in answers:\n",
    "        lst = []\n",
    "        for word in line.split():\n",
    "            if word not in vocab:\n",
    "                lst.append(vocab['<OUT>'])\n",
    "            else:\n",
    "                lst.append(vocab[word])        \n",
    "        decoder_inp.append(lst)\n",
    "    decoder_inp = pad_sequences(decoder_inp, 40, padding='post', truncating='post')\n",
    "    return decoder_inp\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "def build_padded_decoder_final_output(decoder_inp, vocab , inv_vocab):\n",
    "    decoder_final_output = []\n",
    "    for i in decoder_inp:\n",
    "        decoder_final_output.append(i[1:]) \n",
    "    decoder_final_output = pad_sequences(decoder_final_output, 40, padding='post', truncating='post')\n",
    "    decoder_final_output = to_categorical(decoder_final_output, len(vocab))\n",
    "    return(decoder_final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inp  =  build_padded_encoder_input_data(questions , answers , vocab , inv_vocab)\n",
    "decoder_inp = build_padded_decoder_input_data(questions , answers , vocab , inv_vocab)\n",
    "decoder_final_output = build_padded_decoder_final_output(decoder_inp , vocab , inv_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipynb.fs.full.attention import AttentionLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Embedding, Bidirectional, Concatenate, Dropout, Attention\n",
    "from numpy import array\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "with open('/home/parastoo/Downloads/glove.6B(1)/glove.6B.200d.txt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "\n",
    "print(\"Glove Loded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dimention = 200\n",
    "def embedding_matrix_creater(embedding_dimention, word_index):\n",
    "    embedding_matrix = np.zeros((len(word_index)+1, embedding_dimention))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "          # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix\n",
    "embedding_matrix = embedding_matrix_creater(embedding_dimention, word_index=vocab)    \n",
    "print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(vocab)\n",
    "MAX_LEN = 100\n",
    "\n",
    "embed = Embedding(VOCAB_SIZE+1, \n",
    "                  embedding_dimention, \n",
    "                  \n",
    "                  input_length=50,\n",
    "                  trainable=True)\n",
    "\n",
    "embed.build((None,))\n",
    "embed.set_weights([embedding_matrix])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Model_**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_inp = Input(shape=(40, ))\n",
    "#embed = Embedding(VOCAB_SIZE+1, 50, mask_zero=True, input_length=13)(enc_inp)\n",
    "enc_embed = embed(enc_inp)\n",
    "enc_lstm = Bidirectional(LSTM(400, return_state=True, dropout=0.05, return_sequences = True))\n",
    "enc_lstm_2 = Bidirectional(LSTM(400, return_state=True, dropout=0.05, return_sequences = True))\n",
    "encoder_outputs, forward_h, forward_c, backward_h, backward_c = enc_lstm(enc_embed)\n",
    "state_h = Concatenate()([forward_h, backward_h])\n",
    "state_c = Concatenate()([forward_c, backward_c])\n",
    "enc_states_1 = [state_h, state_c]\n",
    "\n",
    "encoder_outputs, forward_h, forward_c, backward_h, backward_c = enc_lstm_2(encoder_outputs)\n",
    "state_h = Concatenate()([forward_h, backward_h])\n",
    "state_c = Concatenate()([forward_c, backward_c])\n",
    "enc_states_2 = [state_h, state_c]\n",
    "\n",
    "dec_inp = Input(shape=(40, ))\n",
    "dec_embed = embed(dec_inp)\n",
    "dec_lstm = LSTM(400*2, return_state=True, return_sequences=True, dropout=0.05)\n",
    "output, _, _ = dec_lstm(dec_embed, initial_state=enc_states_1)\n",
    "dec_lstm_2 = LSTM(400*2, return_state=True, return_sequences=True, dropout=0.05)\n",
    "output, _, _ = dec_lstm_2(output, initial_state=enc_states_2)\n",
    "\n",
    "# attention\n",
    "attn_layer = AttentionLayer()\n",
    "attn_op, attn_state = attn_layer([encoder_outputs, output])\n",
    "decoder_concat_input = Concatenate(axis=-1)([output, attn_op])\n",
    "\n",
    "\n",
    "dec_dense = Dense(VOCAB_SIZE, activation='softmax')\n",
    "final_output = dec_dense(decoder_concat_input)\n",
    "\n",
    "model = Model([enc_inp, dec_inp], final_output)\n",
    "from tensorflow.keras.utils import plot_model\n",
    "plot_model(model, to_file='model_plot4a.png', show_shapes=True, show_layer_names=True)\n",
    "\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "history = model.fit([encoder_inp, decoder_inp], decoder_final_output, epochs= 1\n",
    "          , batch_size=24, validation_split=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all data in history\n",
    "import matplotlib.pyplot as plt\n",
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "enc_model = tf.keras.models.Model(enc_inp, [encoder_outputs, [enc_states_1 , enc_states_2]])\n",
    "\n",
    "\n",
    "decoder_state_input_h_1 = tf.keras.layers.Input(shape=( 400 * 2,))\n",
    "decoder_state_input_c_1 = tf.keras.layers.Input(shape=( 400 * 2,))\n",
    "decoder_state_input_h_2 = tf.keras.layers.Input(shape=( 400 * 2,))\n",
    "decoder_state_input_c_2 = tf.keras.layers.Input(shape=( 400 * 2,))\n",
    "\n",
    "decoder_states_inputs = [decoder_state_input_h_1, decoder_state_input_c_1,\n",
    "                         decoder_state_input_h_2, decoder_state_input_c_2]\n",
    "\n",
    "decoder_outputs, state_h_1, state_c_1 = dec_lstm(dec_embed , \n",
    "                                               initial_state=decoder_states_inputs[:2])\n",
    "decoder_outputs, state_h_2, state_c_2 = dec_lstm_2(decoder_outputs , \n",
    "                                               initial_state=decoder_states_inputs[-2:])\n",
    "\n",
    "decoder_states = [state_h_1, state_c_1,state_h_2, state_c_2]\n",
    "#decoder_output = dec_dense(decoder_outputs)\n",
    "dec_model = tf.keras.models.Model([dec_inp, decoder_states_inputs],\n",
    "                                      [decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "print(\"##########################################\")\n",
    "print(\"#       start chatting ver. 1.0          #\")\n",
    "print(\"##########################################\")\n",
    "\n",
    "\n",
    "prepro1 = \"\"\n",
    "while prepro1 != 'q':\n",
    "    \n",
    "    prepro1 = input(\"you : \")\n",
    "   \n",
    "    prepro1 = preprocess_sentence(prepro1)\n",
    "    prepro = [prepro1]\n",
    "        \n",
    "    txt = []\n",
    "    for x in prepro:\n",
    "        lst = []\n",
    "        for y in x.split():\n",
    "            try:\n",
    "                 lst.append(vocab[y])\n",
    "            except:\n",
    "                lst.append(vocab['<OUT>'])\n",
    "        txt.append(lst)\n",
    "    txt = pad_sequences(txt, 13, padding='post')\n",
    "\n",
    "\n",
    "        ###\n",
    "    enc_op, stat = enc_model.predict( txt )\n",
    "\n",
    "    empty_target_seq = np.zeros( ( 1 , 1) )\n",
    "    empty_target_seq[0, 0] = vocab['<SOS>']\n",
    "    stop_condition = False\n",
    "    decoded_translation = ''\n",
    "\n",
    "#     dead_k = 0 # samples that reached eos\n",
    "#     dead_samples = []\n",
    "#     dead_scores = []\n",
    "#     live_k = 1 # samples that did not yet reached eos\n",
    "#     live_samples = [[empty]]\n",
    "#     live_scores = [0]\n",
    "\n",
    "    while not stop_condition :\n",
    "        dec_outputs , h1, c1 , h2 , c2 = dec_model.predict([ empty_target_seq ] + stat )\n",
    "            ###\n",
    "            ###########################\n",
    "        attn_op, attn_state = attn_layer([enc_op, dec_outputs])\n",
    "        decoder_concat_input = Concatenate(axis=-1)([dec_outputs, attn_op])\n",
    "        decoder_concat_input = dec_dense(decoder_concat_input)\n",
    "            ###########################\n",
    "        print(type(dec_outputs))\n",
    "        sampled_word_index = np.argmax( decoder_concat_input[0, -1, :] )\n",
    "\n",
    "        sampled_word = inv_vocab[sampled_word_index] + ' '\n",
    "\n",
    "        if sampled_word != '<EOS> ':\n",
    "            decoded_translation += sampled_word           \n",
    "\n",
    "\n",
    "        if sampled_word == '<EOS> ' or len(decoded_translation.split()) > 13:\n",
    "            stop_condition = True\n",
    "\n",
    "        empty_target_seq = np.zeros( ( 1 , 1 ) )  \n",
    "        empty_target_seq[ 0 , 0 ] = sampled_word_index\n",
    "        stat = [ h1 , c1 , h2 ,c2 ] \n",
    "    print(\"chatbot attention : \", decoded_translation )\n",
    "    print(\"==============================================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
